---
title: NPU vs. NPUX
author: Niklas Heim
date: 4th May 2020

options:
        doctype: md2html
---

```julia echo=false
using DrWatson
@quickactivate "NIP_2020_NMUX"

using Parameters
using ValueHistories
using Flux
using NeuralArithmetic
using Plots
pyplot()
include(srcdir("arithmetic_dataset.jl"))
```

The complex neural power unit is defined exactly like the **Neural Arithmetic
Logic Unit** (NALU), just that it uses complex valued weight matrices:
```math
z = \exp(W \cdot \log(x + 0i))
```
In julia it is defined by
```julia
const ComplexMatrix = AbstractArray{Complex{Float32},2}

struct NPUX
    W::ComplexMatrix
end

NPUX(in::Int, out::Int, init=Flux.glorot_uniform) = NPUX(init(out,in) .+ 0im)

(m::NPUX)(x::AbstractArray{<:Complex}) = exp.(m.W * log.(x))
(m::NPUX)(x::AbstractArray) = m(x .+ 0im)
```

We can train it to multiply two subsets of a 10 dimensional vector:
```math
y = \sum_{i=1}^{N=5} x_i * \sum_{i=4}^{N=9} x_i
```
with a simple loss function defined by
```julia eval=false
function loss(x,y)
    ŷ = model(x)
    sum(abs.(ŷ .- y))
end
```
which uses the absolute value $|z| = \sqrt{\text{Re}(z)^2 + \text{Im}(z)^2}$

The experiments are run with the following configuration:
```julia
@with_kw struct Config
    batch::Int      = 1000
    inlen::Int      = 10
    niters::Int     = 1000
    lr::Real        = 1e-3
    lowlim::Real    = -2
    uplim::Real     = 2
    subset::Real    = 0.5f0
    overlap::Real   = 0.25f0
end;
```

```julia echo=false
function run(c::Config, model)
    generate = arithmetic_dataset(*, c.inlen,
        d=Uniform(c.lowlim,c.uplim),
        subset=c.subset,
        overlap=c.overlap)
    test_generate = arithmetic_dataset(*, c.inlen,
        d=Uniform(c.lowlim-4,c.uplim+4),
        subset=c.subset,
        overlap=c.overlap)

    function loss(x,y)
        ŷ = model(x)
        # TODO: try with abs . real:
        #       sum(abs.(real.(ŷ .- y)))
        sum(abs.(ŷ .- y))
    end

    data     = (generate(c.batch) for _ in 1:c.niters)
    opt      = RMSProp(c.lr)
    ps       = params(model)

    (x,y) = generate(100)
    (tx,ty) = test_generate(1000)
    callbacks = [
        Flux.throttle(() -> (
        train_loss = loss(x,y)/100;
        valid_loss = loss(tx,ty)/1000;
        @info("Rerun `weave` to get rid of logs", train_loss, valid_loss)), 1)
    ]

    history  = Flux.train!(loss, ps, data, opt, cb=callbacks)

    return @dict(model, history)
end;
```

Unfortunately, with the complex NPU we obtain complex gradients in all other
layers as well, which means we have to convert the whole network to use complex
numbers:
```julia
config = Config()
nau = Flux.fmap(ComplexMatrix, NAU(config.inlen,config.inlen))
model = Chain(nau, NPUX(config.inlen,1))
(res,_) = produce_or_load(
    prefix="npux",
    datadir("jmds"),
    config,
    c -> run(c, model)
)

m = res[:model]
h = res[:history]

h,w = config.inlen, config.inlen
display(heatmap(real.(m[1].W[end:-1:1,:]), title=summary(m[1]), c=:bluesreds, clim=(-1,1)))
display(heatmap(real.(m[2].W[end:-1:1,:]), title=summary(m[2]), c=:bluesreds, clim=(-1,1)))
```


We can do the same experiment with the real NPU layer, which saves us from
converting the whole network to complex arrays:
```julia
config = Config()
model = Chain(NAU(config.inlen,config.inlen), NPU(config.inlen,1))
(res,_) = produce_or_load(
    prefix="npu",
    datadir("jmds"),
    config,
    c -> run(c, model)
)

m = res[:model]
h = res[:history]

h,w = config.inlen, config.inlen
display(heatmap(real.(m[1].W[end:-1:1,:]), title=summary(m[1]), c=:bluesreds, clim=(-1,1)))
display(heatmap(real.(m[2].W[end:-1:1,:]), title=summary(m[2]), c=:bluesreds, clim=(-1,1)))
```
