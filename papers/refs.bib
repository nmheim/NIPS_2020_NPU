
@article{madsen_neural_2020,
	title = {Neural {Arithmetic} {Units}},
	language = {en},
	author = {Madsen, Andreas and Johansen, Alexander Rosenberg},
	year = {2020},
	pages = {31},
	file = {Madsen and Johansen - 2020 - NEURAL ARITHMETIC UNITS.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/E5Y355NV/Madsen and Johansen - 2020 - NEURAL ARITHMETIC UNITS.pdf:application/pdf}
}

@article{trask_neural_2018,
	title = {Neural {Arithmetic} {Logic} {Units}},
	url = {http://arxiv.org/abs/1808.00508},
	abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
	language = {en},
	urldate = {2019-09-17},
	journal = {arXiv:1808.00508 [cs]},
	author = {Trask, Andrew and Hill, Felix and Reed, Scott and Rae, Jack and Dyer, Chris and Blunsom, Phil},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00508},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/AN8CXZYW/Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf:application/pdf}
}

@article{rackauckas_differentialequationsjl_2017,
	title = {{DifferentialEquations}.jl – {A} {Performant} and {Feature}-{Rich} {Ecosystem} for {Solving} {Differential} {Equations} in {Julia}},
	volume = {5},
	issn = {2049-9647},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.151/},
	doi = {10.5334/jors.151},
	language = {en},
	urldate = {2020-04-07},
	journal = {Journal of Open Research Software},
	author = {Rackauckas, Christopher and Nie, Qing},
	month = may,
	year = {2017},
	pages = {15},
	file = {Rackauckas and Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/YGABHTNL/Rackauckas and Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf:application/pdf}
}

@article{kaiser_neural_2016,
	title = {Neural {GPUs} {Learn} {Algorithms}},
	url = {http://arxiv.org/abs/1511.08228},
	abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:1511.08228 [cs]},
	author = {Kaiser, Łukasz and Sutskever, Ilya},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.08228},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Kaiser and Sutskever - 2016 - Neural GPUs Learn Algorithms.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/Q8KJHJMR/Kaiser and Sutskever - 2016 - Neural GPUs Learn Algorithms.pdf:application/pdf}
}

@article{faber_neural_2020,
	title = {Neural {Status} {Registers}},
	url = {http://arxiv.org/abs/2004.07085},
	abstract = {Neural networks excel at approximating functions and ﬁnding patterns in complex and challenging domains. Yet, they fail to learn simple but precise computation. Recent work addressed the ability to add, subtract, and multiply numbers but is lacking a component to drive control ﬂow. True computer intelligence should also be able to decide when to perform what operation. In this paper, we introduce the Neural Status Register (NSR), inspired by physical Status Registers. At the heart of the NSR are arithmetic comparisons between inputs. With theoretically principled changes to physical Status Registers, the NSR allows end-toend diﬀerentiation and learns such comparisons reliably. But the NSR also extrapolates: it generalizes to unseen data distributions. For example, the NSR trains on single digits and correctly predicts numbers that are up to 14 orders of magnitude larger. This suggests that the NSR captures the true underlying arithmetic. In follow-up experiments, we use the NSR to control the computation of a downstream arithmetic unit to learn piecewise functions. We can also learn more challenging tasks through redundancy. Finally, we use the NSR to learn an upstream convolutional neural network to compare images of MNIST digits to decide which image contains the larger digit.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:2004.07085 [cs, stat]},
	author = {Faber, Lukas and Wattenhofer, Roger},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.07085},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Faber and Wattenhofer - 2020 - Neural Status Registers.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/HAU3KHJF/Faber and Wattenhofer - 2020 - Neural Status Registers.pdf:application/pdf}
}

@book{dehaene_number_2011,
	title = {The {Number} {Sense}: {How} the {Mind} {Creates} {Mathematics}, {Revised} and {Updated} {Edition}},
	isbn = {978-0-19-991039-7},
	shorttitle = {The {Number} {Sense}},
	abstract = {Our understanding of how the human brain performs mathematical calculations is far from complete, but in recent years there have been many exciting breakthroughs by scientists all over the world. Now, in The Number Sense, Stanislas Dehaene offers a fascinating look at this recent research, in an enlightening exploration of the mathematical mind. Dehaene begins with the eye-opening discovery that animals--including rats, pigeons, raccoons, and chimpanzees--can perform simple mathematical calculations, and that human infants also have a rudimentary number sense. Dehaene suggests that this rudimentary number sense is as basic to the way the brain understands the world as our perception of color or of objects in space, and, like these other abilities, our number sense is wired into the brain. These are but a few of the wealth of fascinating observations contained here. We also discover, for example, that because Chinese names for numbers are so short, Chinese people can remember up to nine or ten digits at a time--English-speaking people can only remember seven. The book also explores the unique abilities of idiot savants and mathematical geniuses, and we meet people whose minute brain lesions render their mathematical ability useless. This new and completely updated edition includes all of the most recent scientific data on how numbers are encoded by single neurons, and which brain areas activate when we perform calculations. Perhaps most important, The Number Sense reaches many provocative conclusions that will intrigue anyone interested in learning, mathematics, or the mind. "A delight." --Ian Stewart, New Scientist "Read The Number Sense for its rich insights into matters as varying as the cuneiform depiction of numbers, why Jean Piaget's theory of stages in infant learning is wrong, and to discover the brain regions involved in the number sense." --The New York Times Book Review "Dehaene weaves the latest technical research into a remarkably lucid and engrossing investigation. Even readers normally indifferent to mathematics will find themselves marveling at the wonder of minds making numbers." --Booklist},
	language = {en},
	publisher = {Oxford University Press},
	author = {Dehaene, Stanislas},
	month = apr,
	year = {2011},
	note = {Google-Books-ID: 1p6XWYuwpjUC},
	keywords = {Mathematics / General, Psychology / Cognitive Psychology \& Cognition, Science / Life Sciences / Neuroscience}
}

@article{gallistel_finding_2018,
	title = {Finding numbers in the brain},
	volume = {373},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0119},
	doi = {10.1098/rstb.2017.0119},
	language = {en},
	number = {1740},
	urldate = {2020-05-16},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Gallistel, C. R.},
	month = feb,
	year = {2018},
	pages = {20170119},
	file = {Gallistel - 2018 - Finding numbers in the brain.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/6M4BXWVF/Gallistel - 2018 - Finding numbers in the brain.pdf:application/pdf}
}

@article{lake_generalization_2018,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	shorttitle = {Generalization without systematicity},
	url = {http://arxiv.org/abs/1711.00350},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	urldate = {2020-05-16},
	journal = {arXiv:1711.00350 [cs]},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = jun,
	year = {2018},
	note = {arXiv: 1711.00350},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Published at the 35th International Conference on Machine Learning (ICML 2018)},
	file = {arXiv Fulltext PDF:/home/niklas/snap/zotero-snap/common/Zotero/storage/USMWTJ7R/Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:application/pdf;arXiv.org Snapshot:/home/niklas/snap/zotero-snap/common/Zotero/storage/9PL5IRGH/1711.html:text/html}
}

@article{suzgun_evaluating_2018,
	title = {On {Evaluating} the {Generalization} of {LSTM} {Models} in {Formal} {Languages}},
	language = {en},
	author = {Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
	year = {2018},
	pages = {10},
	file = {Suzgun et al. - On Evaluating the Generalization of LSTM Models in.pdf:/home/niklas/snap/zotero-snap/common/Zotero/storage/9UELWSSJ/Suzgun et al. - On Evaluating the Generalization of LSTM Models in.pdf:application/pdf}
}
