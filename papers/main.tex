\documentclass[9pt]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{todonotes}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{graphicx}

\graphicspath{{./../../plots/}}
\newcommand{\real}{\text{real}}

\usepackage{booktabs}
\renewcommand{\arraystretch}{1.2}

\title{Neural Power Unit \\-- A Bayesian Approach To Neural Arithmetic}
\author{
  Niklas Heim,
  V\'aclav \v Sm\'idl,
  Tom\'a\v s Pevn\'y \\
  Artificial Intelligence Center\\
  Czech Technical University\\
  Prague, CZ 120 00\\
  \texttt{\{niklas.heim, vasek.smidl, tomas.pevny\}@aic.fel.cvut.cz}\\
}

\begin{document}

\maketitle

\begin{abstract}
  Common Neural Networks can approximate simple arithmetic operations, but fail
  to generalize well beyond the training manifold. A new class of units called
  \emph{Neural Arithmetic Units} aim to overcome this difficulty, but are limited
  either to operate on positive numbers (NALU by
  \citet{trask_neural_2018}), or can only represent simple addition and
  multiplication (NAU \& NMU by \citet{madsen_neural_2020}).
  We present the first Neural Arithmetic Unit that operates on the full domain
  of real numbers and is capable of learning arbitrary power functions.
\end{abstract}


\section{Introduction}%
\label{sec:introduction}

Neural Networks (NN) have proven to perform exceptionally well at function
approximation tasks\todo{cite universal approx}. This great approximation
performance unfortunately comes at the cost of generalization beyond the
training domain\todo{cite something?}.  The inability to generalize to unseen
inputs is a fundamental problem that hints at a lack of \emph{understanding} of
the given task. The model merely memorizes the seen inputs and fails to
abstract the true learning task.
This can be demonstrated by training a traditional, dense NN to approximate
three functions
\begin{align}
  \label{eq:approx_tasks}
  f(x) = e^x, && g(x) = \log(x), && h(x) = \sin(x)
\end{align}

\begin{figure}
  \centering
  \input{../plots/exp_log_sin/dense_exp.tex}
  \caption{}%
  \label{fig:}
\end{figure}

Neural Arithmetic Units aim to overcome this problem with units that are able
to represent simple arithmetic operations exactly. By composition of these
units it becomes possible to learn e.g. exponentials, logarithms, and power
functions. This promises to improve on the extrapolation capabilities of NNs
even on tasks that are beyond artificial arithmetic tasks such as learning
multiplication, or a certain power function \todo{cite some examples}.
\todo{mention ODEs as use case}



\section{Related Work}%
\label{sec:related_work}

\citet{trask_neural_2018} have demonstrated how severe the extrapolation
problem is even for the simplest arithmetic operations, such as summing or
multiplying two numbers.  In order to increase the power of abstraction of NNs
they propose a \emph{Neural Arithmetic Logic Unit} (NALU) which is capable of
learning arithmetic addition, subtraction, multiplication, and division
$\{+,-,\times,\div\}$ with stunning extrapolation accuracy.  However, the NALU
comes with the severe limitation not being able to handle negative inputs due
to the logarithm in the multiplication part of the NALU:

\begin{align}
  \label{eq:nalu_add}
  \text{Addition: }       & \bm a = \bm W \bm x
                          & \bm W& = \tanh(\hat{\bm W}) \odot \sigma(\hat{\bm M}) \\
  \label{eq:nalu_mult}
  \text{Multiplication: } & \bm m = \exp \bm W(\log(|\bm x|+\epsilon)) & &\\
  \text{Output: }         & \bm y = \bm a \odot \bm g + \bm m \odot (1-\bm g) 
                          & \bm g& = \sigma(\bm G\bm x)
\end{align}

A multiplication layer that can handle negative inputs was introduced by \citet{madsen_neural_2020}.
The \emph{Neural Multiplication Unit} (NMU) is defined by Eq.~\ref{eq:nmu} and is typically used in
conjunction with the so-called \emph{Neural Addition Unit} (NAU) in Eq.~\ref{eq:nau}.
\begin{align}
  \label{eq:nmu}
  \text{NMU: } &y_j = \prod_i M_{ij} z_{i} + 1 - M_{ij}  &M_{ij}=\min(\max(M_{ij}, 0), 1)\\
  \label{eq:nau}
  \text{NAU: } &\bm y = \bm A \bm x &A_{ij}=\min(\max(A_{ij}, 0), 1)
\end{align}
In both NMU and NAU the weights are clipped to $[0,1]$, and typically regularized
with $\mathcal{R}$:
\begin{align}
  \label{eq:rsparse}
  \mathcal{R} = \sum_{ij} \min(W_{ij}, 1-W_{ij})
\end{align}

The combination of NAU and NMU can thus learn $\{+,-,\times\}$, but no division.

\todo{put this in acknowledgements
all the results in this paper were create with the help of the following Julia
packages: \cite{rackauckas_differentialequationsjl_2017} + (Flux.jl)}
\todo{add sqrt and positive range to table~\ref{tab:compare_npu_npu_nalu}?}
\begin{table}
  \centering
  \caption{Comparison of different $2\times2$ layers on the task
  $(x,y)\rightarrow(xy,\frac{x}{y})$.  $(x,y) \in \mathcal U^2(-2,2)$.}
  \label{tab:compare_npu_npu_nalu}
  \input{table_x*y_xdivy.tex}
\end{table}

\section{Neural Power Unit}%
\label{sec:neural_power_unit}

Our \emph{Neural Power Unit} (NPU) can learn arbitrary power functions (which
includes division) while still being able to correctly deal with negative
inputs.  The NPU layer is defined by
\begin{align}
  k_i &= \begin{cases}
     0  & x_i \leq 0 \\
    \pi & x_i > 0
  \end{cases} \\
  \bm r &= |\bm x| \\
  \bm m &= \exp(\bm M \log(\bm r)) \odot \cos(\bm M \bm k)
\end{align}
where $\bm k$ is a vector that is zero where $\bm x$ is positive and $\pi$
where it is negative, $\bm r$ is the elementwise absolute value, and $\bm M$
the multiplication matrix that we want to learn.

The NPU is inspired by the multiplication part of the NALU
Eq.~\ref{eq:nalu_mult}, extended to negative inputs by taking advantage of the
complex logarithm. In general, for any complex number $z$\footnote{Note that
complex numbers can be represented in the polar, complex plane where
$z=re^{i\varphi}$ with $r>0$ and $\varphi \in (0,2\pi)$.}
\begin{align}
  \log(z) = \log\left(r\cdot e^{i\varphi}\right)
     = \log(r) + i\varphi.
\end{align}
However, as $z$ is assumed to be real, we can simplify as follows:
\begin{align}
  \log(z) = \log(r) + ik\pi,
\end{align}
where $k \in [0,1]$ is zero for positive inputs and one for negative inputs $z$.

\begin{align}
  \bm m &= \exp(\bm M \log(\bm x)) \\
    &= \exp(\bm M ( \log(\bm r) + i\pi\bm k ))
\end{align}

\begin{align}
  \bm m_{\text{re}} &= \real(\exp(\bm M \log \bm r + i\pi\bm M\bm k)) \\
    &= \real(\exp(\bm M \log \bm r) \odot (\cos(\pi \bm M \bm k) + i \sin(\pi \bm M \bm k))) \\
    &= \exp(\bm M \log \bm r) \odot \cos(\pi \bm M \bm k)
\end{align}

\section{Experimetns}%
\label{sec:experimetns}

\subsection{10 param func}%
\label{sub:10_param_func}


% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-task-nmu-paper.pdf}
%   \caption{MSE optimization result}%
%   \label{fig:10-param-func-task-nmu-paper}
% \end{figure}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-bayes-task-nmu-paper.pdf}
%   \caption{ARD result with MSE starting point}%
%   \label{fig:10-param-func-bayes-task-nmu-paper}
% \end{figure}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-bayes-task-nmu-paper-history.pdf}
%   \caption{ARD history}%
%   \label{fig:history}
% \end{figure}
% 
% \subsection{10 param func with sqrt and power}%
% \label{sub:10_param_func_with_sqrt_and_power}
% 
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-task-sqrt-power.pdf}
%   \caption{10-param-func-task-sqrt-Power}%
%   \label{fig:10-param-func-task-sqrt-power}
% \end{figure}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-bayes-task-sqrt-power.pdf}
%   \caption{10-param-func-bayes-task-sqrt-Power}%
%   \label{fig:10-param-func-bayes-task-sqrt-power}
% \end{figure}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\linewidth]{10-param-func-bayes-task-sqrt-power-history.pdf}
%   \caption{10-param-func-bayes-task-sqrt-Power-history}%
%   \label{fig:10-param-func-bayes-task-sqrt-power-history}
% \end{figure}


\subsection{Gradient surfaces}%
\label{sub:gradient_surfaces}
% \begin{figure}
%   \centering
%   %\includegraphics[width=0.8\linewidth]{../plots/test.pgf}
%   \input{../plots/test.tikz.tex}
%   \caption{Test}%
%   \label{fig:test}
% \end{figure}

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
